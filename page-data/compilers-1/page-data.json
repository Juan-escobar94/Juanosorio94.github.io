{"componentChunkName":"component---src-templates-blog-post-js","path":"/compilers-1/","result":{"data":{"site":{"siteMetadata":{"title":"Juan Osorio"}},"markdownRemark":{"id":"9680e048-31c0-5672-83a3-d53f394007a0","excerpt":"The Goal of the Compiler The goal of a compiler is just to take a source code as an input and translate it into\na target machine code (e.g. x86 Assembly), which…","html":"<h2>The Goal of the Compiler</h2>\n<p>The goal of a compiler is just to take a source code as an input and translate it into\na target machine code (e.g. x86 Assembly), which is not even yet the object code that\ngets executed on the cpu. After compiling, the assembler and the linker take care of\nthat task.</p>\n<h2>Compiler phases</h2>\n<p>In order to achieve its goal, compilers have been designed into phases, One great benefit\nabout doing this is that we get a lot of modularity. We can create new compilers\nby interchanging reusable modules.</p>\n<h3>Phase 1: Lexical Analysis</h3>\n<p>the first phase when a program is fed to a compiler is\nanalysing the stream of characters and groupping them\ninto meaningful sequences called lexemes. </p>\n<p>for each of the lexemes, the lexical analyzer produces tokens as\noutputs (passed further down to the next step: syntax analysis),\nroupping them as (token name, attribute value), where the\ntoken name is an abstract symbol that is used during syntax analysis,\nthe attribute value points to an entry in the symbol table for this\ntoken. Information from the symbol table entry is needed for semantic\nanalysis and code generation.</p>\n<p>Some tokens do not need attribute values to identify them inside\nthe symbol table, e.g. the equals sign in an assignment. the variable,\nhowever, could produce something like (id, 2), the equals sign (=, _)</p>\n<h3>Syntax Analysis</h3>\n<p>The syntax analysis, also known as parsing, is the second phase of the\ncompiler in which it uses the output from the first phase (tokens)\nto create a tree like intermediate representation that corresponds to\nthe grammatical structure of the token stream. </p>\n<h3>Semantic Analysis</h3>\n<p>The semantic analyzer takes the output from the previous step and also\nthe information from the symbol table (this is present in all steps),\nand checks against the language definition for semantic consistency.</p>\n<p>Type information is also gathered and stored inside the symbol table\nor inside the syntax tree. Type checking is also part of the semantic\nanalysis, where the compiler checks that each operator has matching\noperands.</p>\n<h3>Intermediate Code Generation</h3>\n<h3>Code Optimization</h3>\n<h3>Code Generation</h3>\n<h2>Symbol Table Management</h2>\n<p>The compiler records variable names found in the source program and\ncollects information about various attributes of each name -for instance,\ninformation about the allocated storeage for a name, its type, scope,\nand in case of procedures, number and types of arguments, the method of\npassing the arguments, and the type returned.</p>\n<h2>Compiler Construction Tools</h2>\n<ol>\n<li>Parser generators: produce syntax analyzers from a\ngrammatical description of a language.</li>\n<li>Scanner generators: produce lexical analyzers from a regular-expression\ndescription of the tokens of a language.</li>\n<li>Syntax-directed translation engines: produce a collection of routines\nfor walking a parse tree and generating intermediate code.</li>\n<li>Code-generators generators: produce code generators from a collection\nof rules for translating each operation of the intermediate language into\nthe the machine language for a target machine.</li>\n<li>Data-flow analysis engines: facilitate the gathering of information\nabout how values are transmitted from one part of a program to each other\npart. Data-flow analysis is a key part of code optimization.</li>\n<li>Compiler-construction toolkits that provide an integrated set of\nroutines for constructing various phases of a compiler.</li>\n</ol>\n<h2>Context Free Grammars</h2>\n<p>Used to specify the syntax of a language, a context free grammar has four\ncomponents:</p>\n<ol>\n<li>A set of terminal symbols, also referred as tokens, they are the\nelementary symbols of the language defined by the grammar.</li>\n<li>A set of nonterminals, sometimes called “syntactic variables”,\neach nonterminal represents a set of string of terminals.</li>\n<li>A set of productions, where each one consists of a nonterminal, called\nthe head or left side of the production, an arrow and a sequence of\nterminals and/or nonterminals, called the body or right side of the\nproduction</li>\n<li>A designation of one of the nonterminals as the start symbol.</li>\n</ol>\n<h2>Parsing</h2>\n<p>Parsing is the problem of taking a string of terminals and figuring out how\nto derive it from the start symbol of the grammar and if it cannot be\nderived from the start symbol of the grammar, then reporting syntax errors\nwithin the string.</p>\n<h2>Ambiguity</h2>\n<p>A grammar can have more than one parsing tree generating a given string of\nterminals, such a grammar is said to be ambiguous. To show that a grammar\nis ambiguous, it suffices to find a terminal string that can yield more than\none parsing tree.</p>\n<h2>Associtivity of operators</h2>\n<p>When an operand in a mathematical expression sits in the middle, e.g. 5 in\n4 - 5 + 6, we need to stick to a convention to help us decide the outcome\nof the math expression. In most programming languages, the four common\nmathematical operators are <em>left associative</em> meaning that an operand within\nto plus signs for example belongs to the operator on the left.</p>\n<p>Operators like exponentiation are right associative, as well as the the\nassignment operator we see in languages like C.</p>\n<p>Parse trees for left associative grammars grow (nest) towards the left,\nwhile in right associative grammars, they grow (nest) towards the right.</p>\n<p>a great hint is given, by which which of the non terminal symbols can be\nexpanded (most likely in some kind of a recursive fashion) for a given\nproduction. If its the one on the right, there is a high likelihood it is\na right associative grammar.</p>\n<p>Operators can also have precedence. We say an operator has higher precendence\nthan another operator if it takes it operands before the one with the lower\nprecedence does (* takes for example its operands before + does).</p>\n<p>excerpt from book about grammars that implement precedence level:\nWe can generalize this idea to any number of n precedence levels.\nWe need n+1 nonterminals. The first (highest precedence) can never be torn\napart. Typically, the production bodies for this nonterminal are only single\noperands and parenthesized expressions.</p>\n<h2>Syntax directed translation</h2>\n<p>Syntax directed translation is done by attaching rules or program fragments\nto productions in a grammar. </p>","frontmatter":{"title":"Compilers introduction","date":"February 06, 2021","description":"Post about what I learn while studying compiler construction lecture at TU Dresden"}}},"pageContext":{"slug":"/compilers-1/","previous":{"fields":{"slug":"/code-highlighting-post/"},"frontmatter":{"title":"Syntax Highlighting Post"}},"next":{"fields":{"slug":"/Syzkaller/"},"frontmatter":{"title":"What is Syzkaller, how does it work?"}}}},"staticQueryHashes":["1049812305","2841359383"]}